{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pathlib\n",
    "import time\n",
    "import json\n",
    "from libcomcat.search import search, get_event_by_id\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "In this notebook we will explore the pandas Python library, and how it can speed up data analysis.\n",
    "\n",
    "# Table of contents\n",
    "1. [Defining Data](#defining-data)\n",
    "2. [Loading Data](#loading-data)\n",
    "3. [Examining Data](#examining-data)\n",
    "4. [Selecting Data](#selecting-data)\n",
    "    1. [Series Object](#series-object)\n",
    "    2. [Series Data](#series-data)\n",
    "    3. [Index Definition](#index-definition)\n",
    "    4. [Selecting Data by Column](#selecting-data-by-column)\n",
    "    5. [Selecting Data by Row](#selecting-data-by-row)\n",
    "5. [Create Significant Events](#create-significant-events)\n",
    "    1. [Removing Missing Data](#removing-missing-data)\n",
    "    2. [Select Rows by Multiple Criteria](#select-rows-by-multiple-criteria)\n",
    "6. [Saving Results](#saving-results)\n",
    "7. [Plotting Results](#plotting-results)\n",
    "8. [Working with String Data](#working-with-string-data)\n",
    "9. [Applying Functions to Data](#applying-functions-to-data)\n",
    "10. [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data  <a name=\"defining-data\"></a>\n",
    "\n",
    "Philosophically, the term \"data\" is heavily overloaded:\n",
    "\n",
    " - To a scientist, data can mean raw information collected by a human or instrument in the lab or field\n",
    " - To a programmer, who lives in a binary world, data are those bits of information that are NOT CODE.\n",
    " - To a GIS analyst, data is **geospatial data** - vectors and rasters.\n",
    " - To a business analyst, data can mean (sometimes sensitive) information collected about customers\n",
    "\n",
    "When we talk about \"data\" in the context of this tutorial, we are talking about two-dimensional tabular information, like you would see in a spreadsheet or CSV file. These data could be data collected in the field, the results of a model, the results of a geospatial analysis, or information about customers that may be sensitive or be subject to privacy concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data <a name=\"loading-data\"></a>\n",
    "\n",
    "pandas provides a number of methods for getting data into our structure of primary interest, the *dataframe*.\n",
    "\n",
    " - Excel\n",
    " - CSV/tab separated/other delimiter\n",
    " - Relational Databases (MySQL, SQLite, etc)\n",
    " - Fixed width format data\n",
    " - Manual creation\n",
    " \n",
    " I've written a function here to grab product \"properties\" from ComCat, using it's API. The function saves these \n",
    " results in a CSV. We'll load that CSV file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_file = 'events_2020_properties.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties_dataframe(starttime, endtime, products, \n",
    "                             minmag=5.5, maxmag=9.9):\n",
    "    '''\n",
    "    Download product properties for input products.\n",
    "    \n",
    "    Args:\n",
    "        starttime (datetime): Python datetime for search start time.\n",
    "        endtime (datetime): Python datetime for search end time.\n",
    "        products (list): List of strings of valid ComCat product types \n",
    "                        ('shakemap', 'losspager', 'dyfi', 'finite-fault')\n",
    "        minmag (float): Minimum magnitude for search.\n",
    "        maxmag (float): Maximum magnitude for search.\n",
    "    Returns:\n",
    "        dataframe: pandas Dataframe with fixed columns:\n",
    "                   - eventid\n",
    "                   - time\n",
    "                   - location\n",
    "                   - latitude\n",
    "                   - longitude\n",
    "                   - depth\n",
    "                   - magnitude\n",
    "                   \n",
    "                   and then any number of variable columns of the different product properties.\n",
    "    '''\n",
    "    TIMEFMT = '%Y-%m-%dT%H:%M:%S'\n",
    "    starttime_str = starttime.strftime(TIMEFMT)\n",
    "    endtime_str = endtime.strftime(TIMEFMT)\n",
    "    url = ('https://earthquake.usgs.gov/fdsnws/event/1/query?'\n",
    "           f'format=geojson&starttime={starttime_str}&'\n",
    "           f'endtime={endtime_str}&orderby=time-asc&'\n",
    "           f'minmagnitude={minmag}&maxmagnitude={maxmag}')\n",
    "    with urlopen(url) as fh:\n",
    "        data = fh.read().decode('utf8')\n",
    "    jdict = json.loads(data)\n",
    "    rows = []\n",
    "    for feature in jdict['features']:\n",
    "        row = {}    \n",
    "        types = feature['properties']['types'].split(',')[1:-1]\n",
    "        ptypes = set(products) & set(types) # types we want intersected w/ what we have\n",
    "        if not len(ptypes):\n",
    "            mag = feature['properties']['mag']\n",
    "            continue\n",
    "        detail_url = feature['properties']['detail']\n",
    "        with urlopen(detail_url) as fh:\n",
    "            data = fh.read().decode('utf8')\n",
    "        detail = json.loads(data)\n",
    "        row['eventid'] = detail['id']\n",
    "        \n",
    "        # get event time from Unix timestamp in milliseconds\n",
    "        dtime = detail['properties']['time']\n",
    "        row['time'] = datetime.utcfromtimestamp(dtime/1000)\n",
    "        \n",
    "        row['location'] = detail['properties']['place']\n",
    "        row['latitude'] = detail['geometry']['coordinates'][1]\n",
    "        row['longitude'] = detail['geometry']['coordinates'][0]\n",
    "        row['depth'] = detail['geometry']['coordinates'][2]\n",
    "        row['magnitude'] = detail['properties']['mag']\n",
    "        \n",
    "        for ptype in ptypes:\n",
    "            product = detail['properties']['products'][ptype][0]\n",
    "            for pkey in product['properties']:\n",
    "                key = f'{ptype}-{pkey}'\n",
    "                value = product['properties'][pkey]\n",
    "                try:\n",
    "                    value = int(value)\n",
    "                except:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                if value == 'None':\n",
    "                    value = float('nan')\n",
    "                row[key] = value\n",
    "        rows.append(row)\n",
    "    dataframe = pd.DataFrame(rows)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve a local file, if it exists. We'll show the first five rows of the dataframe when we're done. Note that there are a *lot* of columns, probably more than we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathlib.Path(events_file).exists():\n",
    "    t1=time.time()\n",
    "    products = ['shakemap', 'losspager', 'dyfi']\n",
    "    starttime = datetime(2020, 1, 1)\n",
    "    endtime = datetime(2021, 1, 1)\n",
    "    props = get_properties_dataframe(starttime, endtime, products, minmag=5.5)\n",
    "    t2 = time.time()\n",
    "    dt2 = t2-t1\n",
    "    print(f'{dt2:.1f} seconds to retrieve dataframe')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # save the props dataframe to csv\n",
    "    props.to_csv(events_file, index=False)\n",
    "else:\n",
    "    props = pd.read_csv(events_file, sep=',', header=0, parse_dates=['time'])\n",
    "props.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Data <a name=\"examining-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the dataframe\n",
    "# props.shape\n",
    "# len(props)\n",
    "\n",
    "# look at the columns\n",
    "# props.columns\n",
    "\n",
    "# look at the indices\n",
    "# props.index\n",
    "\n",
    "# Look at the whole data set\n",
    "# props\n",
    "\n",
    "# Look at a subset of the data (by row)\n",
    "# props.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data <a name=\"selecting-data\"></a>\n",
    "\n",
    "There are multiple ways to select data from a DataFrame in either dimension. Here we'll explain a few of these methods and explain some important attributes along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Series Object <a name=\"series-object\"></a>\n",
    "\n",
    "A pandas DataFrame, as we've seen is made up of rows and columns of data, like a spreadsheet. You might be wondering what data structure is returned if you extract a single column or row. The answer is a **Series** object, which is interesting in it's own right for time series analysis, as just one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = props['magnitude']\n",
    "magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Data <a name=\"series-data\"></a>\n",
    "\n",
    "A Series object is a wrapper around a numpy array, and it implements many of the same basic functions as a numpy array. This numpy array is accessible through the Series `values` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Magnitudes sum: {magnitudes.sum():.2f}')\n",
    "print(f'Magnitudes mean: {magnitudes.mean():.2f}')\n",
    "print(f'Magnitudes std: {magnitudes.std():.2f}')\n",
    "print(f'Magnitudes min: {magnitudes.min():.2f}')\n",
    "print(f'Magnitudes max: {magnitudes.max():.2f}')\n",
    "print(f'Magnitudes array: {magnitudes.values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Definition <a name=\"index-definition\"></a>\n",
    "\n",
    "In the same way that a column name identifies a *column*, an index is a way to uniquely identify a *row*. \n",
    "\n",
    "Indices might be the following things:\n",
    "\n",
    " - In **medicine**, an index might be a patient ID\n",
    " - In **business**, an index might be an invoice ID, or a customer ID\n",
    " - In **seismology**, an index might be the ComCat ID, or a seismic station ID.\n",
    " \n",
    "That being said, I tend not to use the index capability, because searching for rows based on specific criteria is typically a more useful approach - few people internalize or remember ComCat IDs, and you may very easily have more than one row with the same seismic station in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data by Column <a name=\"selecting-data-by-column\"></a>\n",
    "\n",
    "You can select data from one or more dataframe columns, or one or more rows. Row selection can be done in a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting by columns\n",
    "keep_columns = ['eventid', 'time', \n",
    "                'latitude', 'longitude', \n",
    "                'depth', 'magnitude'\n",
    "               ]\n",
    "props[keep_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data by Row <a name=\"selecting-data-by-row\"></a>\n",
    "\n",
    "You can select data by row in three different ways:\n",
    "\n",
    " - Select by index\n",
    " - Select by row location\n",
    " - Select by column criteria\n",
    " \n",
    " In order to demonstrate that the row **index** can be different than row **location**, I'll go out of order and create a subset of data first by selecting events with at least a magnitude of 7.5. Later we'll go into more detail on selecting data from rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by simple criteria\n",
    "mag_criteria = props['magnitude'] >= 7.5\n",
    "subframe = props[mag_criteria]\n",
    "subframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rows in this data subset retained their index. Now let's select rows from this subset by that *index*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subframe.loc[[39,92]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select rows from this subset by row *location*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subframe.iloc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Significant Events <a name=\"create-significant-events\"></a>\n",
    "\n",
    "Let's select down from all of these product properties some that are interesting for further analysis. Here we're selecting the following columns:\n",
    "\n",
    " - **shakemap-maxmmi** The maximum Mercalli intensity for an event as modeled by ShakeMap\n",
    " - **losspager-alertlevel** The PAGER alert level (Green, Yellow, Orange, Red) assigned to the event\n",
    " - **dyfi-num-responses** The number of DYFI responses that came in from users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['eventid', 'time', \n",
    "                'latitude', 'longitude', \n",
    "                'depth', 'magnitude', \n",
    "                'shakemap-maxmmi', 'losspager-alertlevel',\n",
    "                'dyfi-num-responses'\n",
    "               ]\n",
    "sprops = props[keep_columns]\n",
    "sprops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Data <a name=\"removing-missing-data\"></a>\n",
    "\n",
    "Notice above that some earthquakes do not have all columns set to real values - where this occurs, pandas inserts *NaN* values. pandas provides a method to allow us to remove rows with NaNs present in any of the columns. Let's remove all the rows where *any* of our properties of interest is set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up events that do not have DYFI or PAGER results\n",
    "print(f'Events before filter: {len(sprops)}')\n",
    "columns = ['losspager-alertlevel', 'dyfi-num-responses', 'shakemap-maxmmi']\n",
    "sprops = sprops.dropna(subset=columns)\n",
    "print(f'Events after filter: {len(sprops)}')\n",
    "sprops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Rows by Multiple Criteria <a name=\"select-rows-by-multiple-criteria\"></a>\n",
    "\n",
    "Selecting rows based on single or multiple column criteria will be a common goal. Imagine that we want to select down a list of \"significant\" events from 2020, based on four criteria:\n",
    "\n",
    " - **Magnitude** >= 7.0\n",
    " - **Maximum MMI** >= 7.0\n",
    " - **Number of DYFI Responses** >= 100\n",
    " - **PAGER Alert Level** >= Yellow\n",
    " \n",
    " We'll go through each of these in turn, and then see how to combine them to create our dataset of significant events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_magnitude = sprops['magnitude'] >= 6.0\n",
    "sprops.loc[condition_magnitude].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this condition look like? It turns out it is a **Series** object with boolean values at each index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_maxmmi = sprops['shakemap-maxmmi'] >= 6.0\n",
    "sprops.loc[condition_maxmmi].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_numresp = sprops['dyfi-num-responses'] > 100\n",
    "sprops.loc[condition_numresp].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_yellow = sprops['losspager-alertlevel'].str.match('yellow')\n",
    "condition_orange = sprops['losspager-alertlevel'].str.match('orange')\n",
    "condition_red = sprops['losspager-alertlevel'].str.match('red')\n",
    "\n",
    "# you can combine these conditions with \"and\" or \"or\" conditions\n",
    "combined_condition = condition_yellow | condition_orange | condition_red\n",
    "sprops.loc[combined_condition].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_alert = sprops['losspager-alertlevel'].isin(['yellow', 'orange', 'red'])\n",
    "sprops.loc[condition_alert].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine all of these conditions together, creating a list of significant events from 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine them all together\n",
    "# recall: M >= 6 OR maxmmi >= 6 OR NumResp >= 100 OR Alert > GREEN\n",
    "combined_condition = condition_magnitude & condition_maxmmi & condition_numresp & condition_alert\n",
    "significant = sprops.loc[combined_condition]\n",
    "significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results <a name=\"saving-results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index=False indicates that we do not want to save the index \"column\" to the output format. Feel free to keep \n",
    "# the index if it is useful in your work.\n",
    "significant.to_csv('significant_events_2020.csv', index=False)\n",
    "significant.to_excel('significant_events_2020.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results <a name=\"plotting-results\"></a>\n",
    "\n",
    "pandas also incorporates matplotlib plotting capabilities within it, so it is possible to make a number of plots from your dataset without calling matplotlib functions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprops['losspager-alertlevel'].plot.bar()\n",
    "ax = sprops['losspager-alertlevel'].value_counts(sort=False).plot.bar(title='Alert Level Counts', figsize=(15,7))\n",
    "# ax.get_xlim()\n",
    "ax.text(1.0, 175, f'N={len(sprops)}', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sprops.plot(x='magnitude', y='shakemap-maxmmi', kind='scatter', figsize=(15,7.5), title = 'Mag vs MMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = props['magnitude'].hist(figsize=(15,7.5));\n",
    "fig = ax.figure\n",
    "fig.suptitle='Magnitude Frequency Distribution'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can always plot the data using matplotlib functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "plt.plot(sprops['magnitude'], sprops['shakemap-maxmmi'], 'b.')\n",
    "tstring = plt.title('Mag vs MMI')\n",
    "xlabel = plt.xlabel('Magnitude')\n",
    "ylabel = plt.ylabel('MMI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with String Data <a name=\"working-with-string-data\"></a>\n",
    "\n",
    "Let's load in a new dataframe to explore what can be done with strings in pandas columns. This table contains earthquake \"pick\" information - that is, the time at which the earthquake wave ('P', 'S', or more exotic flavors like 'PKPdf') was detected at the station. \n",
    "\n",
    "In this case the arrival time information is split up into multiple **string** fields, instead of being in one datetime field as we might expect. Additionally, each date or time column is preceded by a character denoting what time unit is is (\"Y\" for \"Year\", etc.). Let's try to merge these columns into one 'Arrival Time' column which has a datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = pd.read_excel('us60007fd8_phases_broken.xlsx')\n",
    "phases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series object (remember that extracted rows and columns are Series) has an \"accessor\" called \"str\" that allows you to perform string operations on the fields in that Series. Let's use that to fix the date/time columns to be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases['year'] = phases['year'].str.replace('Y','')\n",
    "phases['month'] = phases['month'].str.replace('m','')\n",
    "phases['day'] = phases['day'].str.replace('D','')\n",
    "phases['hour'] = phases['hour'].str.replace('H','')\n",
    "phases['minute'] = phases['minute'].str.replace('M','')\n",
    "phases['second'] = phases['second'].str.replace('S','')\n",
    "phases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timecols = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "phases['Arrival Time'] = pd.to_datetime(phases[timecols])\n",
    "phases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get rid of our unnecessary date/time fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = phases.drop(labels=timecols, axis='columns')\n",
    "phases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions to Data <a name=\"applying-functions-to-data\"></a>\n",
    "\n",
    "Most of the time, you should be able to perform vector operations on your pandas DataFrame. Sometimes, however, you may not be able to do this. Here we'll show the use of the DataFrame `apply()` method to all rows of our data. First, let's write a little function to calculate the average rate the wave traveled.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rate(row):\n",
    "    # hard coding event time b/c it's not in our data set\n",
    "    # 2020-01-25 03:03:34.276\n",
    "    event_time = datetime(2020,1,25,3,3,34)\n",
    "    dt = row['Arrival Time'] - event_time\n",
    "    rate = row['Distance']/dt.total_seconds()\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases['Rate (deg/sec)'] = phases.apply(calc_rate, axis='columns')\n",
    "phases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources <a name=\"resources\"></a>\n",
    "\n",
    " - pandas Documentation: https://pandas.pydata.org/\n",
    " - pandas Read CSV: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    " - Excellent (4 hour) pandas tutorial from Scipy 2021: https://youtu.be/9dz1fmBUF8U?t=66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
